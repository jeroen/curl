% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/multi_download.R
\name{multi_download}
\alias{multi_download}
\title{Download multiple files concurrently}
\usage{
multi_download(
  urls,
  destfiles = NULL,
  resume = FALSE,
  timeout = Inf,
  progress = FALSE,
  ...
)
}
\arguments{
\item{urls}{vector with files to download}

\item{destfiles}{vector (of equal length as \code{urls}) with paths of output files,
or \code{NULL} to use \link{basename} of urls.}

\item{resume}{if the file already exists, resume the download. Note that servers
may return http-416 errors for resources that cannot be resumed or if the download
was already completed (i.e. nothing left to resume).}

\item{timeout}{in seconds, passed to \link{multi_run}}

\item{progress}{print download progress information}

\item{...}{extra handle options passed to each request \link{new_handle}}
}
\description{
Wrapper for \code{\link[=multi_run]{multi_run()}} to download multiple requests concurrently. Also supports
resuming downloads for large files. This function does not error in case any of the
requests fail; instead it returns a data frame with information about the requests.
}
\examples{
urls <- c('https://cran.r-project.org/src/contrib/Archive/V8/V8_4.2.1.tar.gz',
'https://cran.r-project.org/src/contrib/Archive/curl/curl_4.3.2.tar.gz',
'https://urldoesnotexist.xyz/nothing.zip',
'https://github.com/jeroen/curl/archive/refs/heads/master.zip',
'https://httpbin.org/status/418')

multi_download(urls)
}
